Intent recognition using XLM-RoBERTa is a sophisticated approach that harnesses the power of a pre-trained transformer-based language model to accurately classify the intent behind a user's input text. By fine-tuning the model on a labeled dataset tailored to the specific application domain, such as customer service queries or voice commands, we achieved an impressive accuracy of 98.57%. XLM-RoBERTa's multi-lingual capabilities make it exceptionally well-suited for tasks involving diverse languages, while its deep contextual understanding enables accurate intent classification even with limited training data. This methodology offers a robust and efficient solution for understanding user intentions in natural language processing applications. Our achievement of a 98% accuracy on the ATIS Dataset underscores the effectiveness and reliability of this approach in real-world scenarios.

Model Architecture : 
The overall architecture of project is as follows.
![image](https://github.com/jayanth24-kj/Intent-Recognition/assets/103555317/6ba1b255-dd99-45db-b35b-46a85d883234)

Web App Interface:
![image](https://github.com/jayanth24-kj/Intent-Recognition/assets/103555317/4454c305-196a-4e21-93cd-7aea8def40c3)

Classified Interface:
![image](https://github.com/jayanth24-kj/Intent-Recognition/assets/103555317/9ad81fd5-5c58-49ef-b32d-d579e38fc320)

